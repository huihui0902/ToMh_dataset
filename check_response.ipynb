{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "input_folder = 'API_responses'\n",
    "output_folder = 'extracted_response'\n",
    "lengths = [1, 2, 3]\n",
    "orders = [0, 1, 2, 3, 4]\n",
    "prompts = ['CoT', 'MC']\n",
    "tells = ['No_Tell', 'Tell']\n",
    "for tell, prompt, length, order, sample_num in itertools.product(tells, prompts, lengths, orders, range(1, 21)):\n",
    "    input_fn = os.path.join(input_folder, tell, prompt, f'length_{length}', f'sample_{sample_num}',\n",
    "                            f'order_{order}.txt')\n",
    "    output_fn = os.path.join(output_folder, tell, prompt, f'length_{length}', f'sample_{sample_num}',\n",
    "                                f'order_{order}.txt')\n",
    "    with open(input_fn, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        answer_line = lines[0]\n",
    "        answers = answer_line.split()\n",
    "        if 'Answer:' in answers:\n",
    "            answers.remove('Answer:')\n",
    "        extracted = ' '.join(answers)\n",
    "    if not os.path.exists(os.path.join(output_folder, tell, prompt, f'length_{length}', f'sample_{sample_num}')):\n",
    "        os.makedirs(os.path.join(output_folder, tell, prompt,\n",
    "                    f'length_{length}', f'sample_{sample_num}'))\n",
    "    with open(output_fn, 'w') as file:\n",
    "        file.writelines(extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Process GPT-4 data\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "input_folder = 'extracted_response'\n",
    "answer_folder = 'data_ToMh_old'\n",
    "response_folder = 'API_responses'\n",
    "output_folder = 'check_GPT-4'\n",
    "summary_file = 'GPT-4_data.txt'\n",
    "lengths = [1, 2, 3]\n",
    "orders = [0, 1, 2, 3, 4]\n",
    "prompts = ['CoT', 'MC']\n",
    "tells = ['No_Tell', 'Tell']\n",
    "CoT_count, CoT_correct = [0, 0]\n",
    "MC_count, MC_correct = [0, 0]\n",
    "summary_lines = []\n",
    "accuracy_data = {'Tell': {'CoT': np.zeros((3, 5)), 'MC': np.zeros((3, 5))},\n",
    "                'No_Tell': {'CoT': np.zeros((3, 5)), 'MC': np.zeros((3, 5))}}\n",
    "correctness_for_each = {'Tell': {'CoT': np.zeros((3, 5, 20)), 'MC': np.zeros((3, 5, 20))},\n",
    "                'No_Tell': {'CoT': np.zeros((3, 5, 20)), 'MC': np.zeros((3, 5, 20))}}\n",
    "cnt_vs_deception = np.zeros((5, 3))\n",
    "correct_vs_deception = np.zeros((5, 3))\n",
    "\n",
    "for tell, prompt, length, order in itertools.product(tells, prompts, lengths, orders):\n",
    "    out_folder = os.path.join(output_folder, tell, prompt, f'length_{length}', f'order_{order}')\n",
    "    summary_lines.append(f'{tell}, {prompt}, {length} chapters, {order} order: ')\n",
    "    \n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "\n",
    "    for sample_num in range(1, 21):\n",
    "        input_fn = os.path.join(input_folder, tell, prompt, f'length_{length}', f'sample_{sample_num}',\n",
    "                                f'order_{order}.txt')\n",
    "        response_fn = os.path.join(response_folder, tell, prompt, f'length_{length}', f'sample_{sample_num}',\n",
    "                                f'order_{order}.txt')\n",
    "        answer_fn = os.path.join(answer_folder, tell, 'CoT', f'length_{length}', f'sample_{sample_num}',\n",
    "                                    f'order_{order}.txt')\n",
    "        output_fn = os.path.join(output_folder, tell, prompt, f'length_{length}', f'sample_{sample_num}',\n",
    "                                    f'order_{order}.txt')\n",
    "\n",
    "        with open(answer_fn, 'r') as file:\n",
    "            output_lines = file.readlines()\n",
    "        count_deception = 0\n",
    "        with open(answer_fn, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                if 'Answer:' in line.split(' '):\n",
    "                    correct_answer = line.split(' ', 1)[1].split('\\n')[0]\n",
    "                    output_lines.append('Correct answer: ' + correct_answer + '\\n')\n",
    "                if 'privately' in line.split(' ') or 'publicly' in line.split(' '):\n",
    "                    count_deception += 1\n",
    "        cnt_vs_deception[order][count_deception // 2] += 1\n",
    "        \n",
    "        with open(input_fn, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            if len(lines[0].split(' ')) == 2:\n",
    "                answer_line = lines[0].split(' ')[1]\n",
    "                output_lines.append('GPT-4 Answer: ' + answer_line + '\\n')\n",
    "            else:\n",
    "                # print(f\"Eliminated: {tell}, {prompt}, {length} chapters, {order} order, \" , lines)\n",
    "                continue\n",
    "        with open(response_fn, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            output_lines.append('GPT-4 Explain: ')\n",
    "            output_lines += lines\n",
    "        \n",
    "        total_count += 1\n",
    "        if answer_line == correct_answer:\n",
    "            correct_count += 1\n",
    "            correct_vs_deception[order][count_deception // 2] += 1\n",
    "        \n",
    "        if answer_line != correct_answer:\n",
    "            if not os.path.exists(os.path.join(output_folder, tell, prompt, f'length_{length}', f'sample_{sample_num}')):\n",
    "                os.makedirs(os.path.join(output_folder, tell, prompt, f'length_{length}', f'sample_{sample_num}'))\n",
    "            with open(output_fn, 'w') as file:\n",
    "                file.writelines(output_lines)\n",
    "    accuracy = correct_count / total_count\n",
    "    summary_lines.append(str(accuracy) + '\\n')\n",
    "    accuracy_data[tell][prompt][length - 1][order] = accuracy\n",
    "    with open(summary_file, 'w') as file:\n",
    "        file.writelines(summary_lines)\n",
    "pprint.pprint(accuracy_data)\n",
    "pprint.pprint(cnt_vs_deception)\n",
    "pprint.pprint(correct_vs_deception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot accuracy heatmap for GPT-4\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "FONT_SIZE = 25\n",
    "TICK_SIZE = 21\n",
    "LEGEND_FONT_SIZE = 22\n",
    "TITLE_FONT_SIZE = 30\n",
    "\n",
    "plt.rc('font', size=FONT_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=FONT_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=FONT_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=TICK_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=TICK_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=LEGEND_FONT_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=42)  # fontsize of the figure title\n",
    "# plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "\n",
    "for tell, prompt in itertools.product(tells, prompts):\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    im = ax.imshow(accuracy_data[tell][prompt], cmap=mpl.colormaps['Blues'])\n",
    "\n",
    "    # Show all ticks and label them with the respective list entries\n",
    "    ax.set_xticks(np.arange(5), rotation=45)\n",
    "    ax.set_yticks(np.arange(3), labels=[1, 2, 3])\n",
    "    ax.set_xlabel('ToM order')\n",
    "    ax.set_ylabel('Story length')\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    cbar.ax.set_ylabel(\"Accuracy\", rotation=-90, va=\"bottom\")\n",
    "    communication = 'Exist deception' if tell == 'Tell' else 'No deception'\n",
    "    ax.set_title(f'{communication}, {prompt} prompting')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(),\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(3):\n",
    "        for j in range(5):\n",
    "            rounded = np.round(accuracy_data[tell][prompt], 2)\n",
    "            fontcolor = 'black' if accuracy_data[tell][prompt][i][j] < 0.7 else 'white'\n",
    "            text = ax.text(j, i, rounded[i][j],\n",
    "                        ha=\"center\", va=\"center\", color=fontcolor)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f'data_figs/{tell}_{prompt}.pdf')\n",
    "\n",
    "\"\"\"Plot 2 combined heatmaps\"\"\"\n",
    "fig, axs = plt.subplots(1, 2, figsize=(17,8))\n",
    "im1 = axs[0].imshow(accuracy_data['No_Tell']['CoT'], cmap=mpl.colormaps['Blues'])\n",
    "im2 = axs[1].imshow(accuracy_data['Tell']['CoT'], cmap=mpl.colormaps['Blues'])\n",
    "\n",
    "# Show all ticks and label them with the respective list entries\n",
    "for i in range(2):\n",
    "    axs[i].set_xticks(np.arange(5), rotation=45)\n",
    "    axs[i].set_yticks(np.arange(3), labels=[1, 2, 3])\n",
    "    axs[i].set_xlabel('ToM order')\n",
    "    axs[i].set_ylabel('Story length')\n",
    "    communication = 'Exist deception' if i == 1 else 'No deception'\n",
    "    axs[i].set_title(communication, fontsize=TITLE_FONT_SIZE)\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(3):\n",
    "    for j in range(5):\n",
    "        rounded = np.round(accuracy_data['No_Tell']['CoT'], 2)\n",
    "        fontcolor = 'black' if accuracy_data['No_Tell']['CoT'][i][j] < 0.7 else 'white'\n",
    "        text = axs[0].text(j, i, rounded[i][j],\n",
    "                    ha=\"center\", va=\"center\", color=fontcolor)\n",
    "for i in range(3):\n",
    "    for j in range(5):\n",
    "        rounded = np.round(accuracy_data['Tell']['CoT'], 2)\n",
    "        fontcolor = 'black' if accuracy_data['Tell']['CoT'][i][j] < 0.7 else 'white'\n",
    "        text = axs[1].text(j, i, rounded[i][j],\n",
    "                    ha=\"center\", va=\"center\", color=fontcolor)\n",
    "\n",
    "cbar = fig.colorbar(im1, ax=axs, orientation='vertical', shrink=0.6)\n",
    "cbar.ax.set_ylabel('Accuracy', rotation=-90, va=\"bottom\")\n",
    "plt.savefig(f'data_figs/CoT_heatmaps.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Count joint accuracy.\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "input_folder = 'extracted_response'\n",
    "answer_folder = 'data_ToMh_old'\n",
    "response_folder = 'API_responses'\n",
    "output_folder = 'check_GPT-4'\n",
    "summary_file = 'GPT-4_data.txt'\n",
    "lengths = [1, 2, 3]\n",
    "orders = [0, 1, 2, 3, 4]\n",
    "prompts = ['CoT', 'MC']\n",
    "tells = ['No_Tell', 'Tell']\n",
    "CoT_count, CoT_correct = [0, 0]\n",
    "MC_count, MC_correct = [0, 0]\n",
    "summary_lines = []\n",
    "joint_accuracy_data = {'Tell': {'CoT': np.zeros((3, 5)), 'MC': np.zeros((3, 5))},\n",
    "                'No_Tell': {'CoT': np.zeros((3, 5)), 'MC': np.zeros((3, 5))}}\n",
    "joint_cnt_vs_deception = np.zeros((5, 7))\n",
    "joint_correct_vs_deception = np.zeros((5, 7))\n",
    "deception = set()\n",
    "\n",
    "for tell, prompt, length in itertools.product(tells, prompts, lengths):\n",
    "    \n",
    "    total_count = np.zeros(5)\n",
    "    correct_count = np.zeros(5)\n",
    "\n",
    "    for sample_num in range(1, 21):\n",
    "        correct_so_far = True\n",
    "        for i, order in enumerate(orders):\n",
    "            input_fn = os.path.join(input_folder, tell, prompt, f'length_{length}', f'sample_{sample_num}',\n",
    "                                    f'order_{order}.txt')\n",
    "            response_fn = os.path.join(response_folder, tell, prompt, f'length_{length}', f'sample_{sample_num}',\n",
    "                                    f'order_{order}.txt')\n",
    "            answer_fn = os.path.join(answer_folder, tell, 'CoT', f'length_{length}', f'sample_{sample_num}',\n",
    "                                        f'order_{order}.txt')\n",
    "            output_fn = os.path.join(output_folder, tell, prompt, f'length_{length}', f'sample_{sample_num}',\n",
    "                                        f'order_{order}.txt')\n",
    "            count_deception = 0\n",
    "            with open(answer_fn, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "                for line in lines:\n",
    "                    if 'Answer:' in line.split(' '):\n",
    "                        correct_answer = line.split(' ', 1)[1].split('\\n')[0]\n",
    "                    if 'privately' in line.split(' '):\n",
    "                        count_deception += 1\n",
    "                    if 'publicly' in line.split(' '):\n",
    "                        count_deception += 2\n",
    "            \n",
    "            joint_cnt_vs_deception[order][count_deception] += 1\n",
    "            \n",
    "            with open(input_fn, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "                if len(lines[0].split(' ')) == 2:\n",
    "                    answer_line = lines[0].split(' ')[1]\n",
    "                else:\n",
    "                    # print(f\"Eliminated: {tell}, {prompt}, {length} chapters, {order} order, \" , lines)\n",
    "                    continue\n",
    "            \n",
    "            total_count[i] += 1\n",
    "            if answer_line == correct_answer and correct_so_far:\n",
    "                correct_count[i] += 1\n",
    "                joint_correct_vs_deception[order][count_deception] += 1\n",
    "            elif answer_line != correct_answer:\n",
    "                correct_so_far = False\n",
    "    for order in orders:\n",
    "        joint_accuracy_data[tell][prompt][length - 1][order] = correct_count[order] / total_count[order]\n",
    "# print(deception)\n",
    "pprint.pprint(joint_accuracy_data)\n",
    "# pprint.pprint(joint_cnt_vs_deception)\n",
    "# pprint.pprint(joint_correct_vs_deception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot joint accuracy heatmap for GPT-4\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "FONT_SIZE = 25\n",
    "TICK_SIZE = 21\n",
    "LEGEND_FONT_SIZE = 22\n",
    "TITLE_FONT_SIZE = 30\n",
    "\n",
    "plt.rc('font', size=FONT_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=FONT_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=FONT_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=TICK_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=TICK_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=LEGEND_FONT_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=42)  # fontsize of the figure title\n",
    "# plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "\n",
    "for tell, prompt in itertools.product(tells, prompts):\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    im = ax.imshow(joint_accuracy_data[tell][prompt], cmap=mpl.colormaps['Blues'])\n",
    "\n",
    "    # Show all ticks and label them with the respective list entries\n",
    "    ax.set_xticks(np.arange(5), rotation=45)\n",
    "    ax.set_yticks(np.arange(3), labels=[1, 2, 3])\n",
    "    ax.set_xlabel('ToM order')\n",
    "    ax.set_ylabel('Story length')\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    cbar.ax.set_ylabel(\"Accuracy\", rotation=-90, va=\"bottom\")\n",
    "    communication = 'Exist deception' if tell == 'Tell' else 'No deception'\n",
    "    ax.set_title(f'{communication}, {prompt} prompting')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(),\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(3):\n",
    "        for j in range(5):\n",
    "            rounded = np.round(joint_accuracy_data[tell][prompt], 2)\n",
    "            fontcolor = 'black' if joint_accuracy_data[tell][prompt][i][j] < 0.7 else 'white'\n",
    "            text = ax.text(j, i, rounded[i][j],\n",
    "                        ha=\"center\", va=\"center\", color=fontcolor)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f'data_figs/{tell}_{prompt}.pdf')\n",
    "\n",
    "\"\"\"Plot 2 combined heatmaps\"\"\"\n",
    "fig, axs = plt.subplots(1, 2, figsize=(17,8))\n",
    "im1 = axs[0].imshow(joint_accuracy_data['No_Tell']['CoT'], cmap=mpl.colormaps['Blues'])\n",
    "im2 = axs[1].imshow(joint_accuracy_data['Tell']['CoT'], cmap=mpl.colormaps['Blues'])\n",
    "\n",
    "# Show all ticks and label them with the respective list entries\n",
    "for i in range(2):\n",
    "    axs[i].set_xticks(np.arange(5), rotation=45)\n",
    "    axs[i].set_yticks(np.arange(3), labels=[1, 2, 3])\n",
    "    axs[i].set_xlabel('ToM order')\n",
    "    axs[i].set_ylabel('Story length')\n",
    "    communication = 'Exist deception' if i == 1 else 'No deception'\n",
    "    axs[i].set_title(communication, fontsize=TITLE_FONT_SIZE)\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(3):\n",
    "    for j in range(5):\n",
    "        rounded = np.round(joint_accuracy_data['No_Tell']['CoT'], 2)\n",
    "        fontcolor = 'black' if joint_accuracy_data['No_Tell']['CoT'][i][j] < 0.7 else 'white'\n",
    "        text = axs[0].text(j, i, rounded[i][j],\n",
    "                    ha=\"center\", va=\"center\", color=fontcolor)\n",
    "for i in range(3):\n",
    "    for j in range(5):\n",
    "        rounded = np.round(joint_accuracy_data['Tell']['CoT'], 2)\n",
    "        fontcolor = 'black' if joint_accuracy_data['Tell']['CoT'][i][j] < 0.7 else 'white'\n",
    "        text = axs[1].text(j, i, rounded[i][j],\n",
    "                    ha=\"center\", va=\"center\", color=fontcolor)\n",
    "\n",
    "cbar = fig.colorbar(im1, ax=axs, orientation='vertical', shrink=0.6)\n",
    "cbar.ax.set_ylabel('Accuracy', rotation=-90, va=\"bottom\")\n",
    "plt.savefig(f'data_figs/CoT_heatmaps.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Accuracy vs. deception times\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "FONT_SIZE = 25\n",
    "TICK_SIZE = 21\n",
    "LEGEND_FONT_SIZE = 22\n",
    "\n",
    "plt.rc('font', size=FONT_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=FONT_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=FONT_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=TICK_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=TICK_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=LEGEND_FONT_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=42)  # fontsize of the figure title\n",
    "# plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "markers = ['o', 's', '^', 'D', '.']\n",
    "acc_vs_deception = np.zeros((5, 3))\n",
    "\n",
    "for order in range(1, 5):\n",
    "    for deception_num in range(3):\n",
    "        acc_vs_deception[order][deception_num] = correct_vs_deception[order][deception_num] / cnt_vs_deception[order][deception_num]\n",
    "print(acc_vs_deception)\n",
    "\n",
    "colors = ['steelblue', 'orange', 'lightcoral']\n",
    "plt.figure(figsize=(8, 5)) \n",
    "for i, acc in enumerate(acc_vs_deception):\n",
    "    # plt.plot([0, 2, 4], acc, label=[0, 2, 4], marker=markers[i], color=colors[i], linewidth=2.6, markersize=10)\n",
    "    if i == 0:\n",
    "        continue\n",
    "    plt.plot([0, 2, 4], acc, linewidth=2.6, marker=markers[i], markersize=9, label=f'Order {i}')\n",
    "\n",
    "# 添加标题和轴标签\n",
    "# plt.title('Multiple Line Plot')\n",
    "ax = plt.gca()\n",
    "plt.xlabel('Number of deception per story', labelpad=10)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0.1, 0.7)\n",
    "plt.xticks([0, 2, 4])\n",
    "plt.grid(axis='y', linestyle='dashed', which='both')\n",
    "# plt.box(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['bottom'].set_color('silver')\n",
    "plt.tick_params(axis='x', which='both', bottom=False, pad=10)\n",
    "plt.tick_params(left=False)\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.4), ncol=3, frameon=False)\n",
    "plt.subplots_adjust(bottom=0.2, top=0.9)\n",
    "\n",
    "plt.savefig('linechart.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Joint accuracy vs. deception times\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "FONT_SIZE = 25\n",
    "TICK_SIZE = 21\n",
    "LEGEND_FONT_SIZE = 22\n",
    "\n",
    "plt.rc('font', size=FONT_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=FONT_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=FONT_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=TICK_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=TICK_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=LEGEND_FONT_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=42)  # fontsize of the figure title\n",
    "# plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "markers = ['.', 's', '^', 'D', 'o']\n",
    "joint_acc_vs_deception = np.zeros((5, 7))\n",
    "order_label = ['0th', '1st', '2nd', '3rd', '4th']\n",
    "\n",
    "for order in range(5):\n",
    "    for deception_num in [0,2,3,5,6]:\n",
    "        joint_acc_vs_deception[order][deception_num] = joint_correct_vs_deception[order][deception_num] / joint_cnt_vs_deception[order][deception_num]\n",
    "print(joint_acc_vs_deception)\n",
    "\n",
    "joint_acc_vs_deception = joint_acc_vs_deception[:, [0,2,3,5,6]]\n",
    "colors = ['neglect', '#7C84B9', 'orange', 'lightcoral', 'steelblue']\n",
    "plt.figure(figsize=(9, 5.5)) \n",
    "for i, acc in enumerate(joint_acc_vs_deception):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    zorder = 3 if i == 3 else 2\n",
    "    plt.plot([0,1,2,3,4], acc, linewidth=2.6, marker=markers[i], markersize=9, label=order_label[i], color=colors[i], zorder=zorder)\n",
    "\n",
    "# 添加标题和轴标签\n",
    "# plt.title('Multiple Line Plot')\n",
    "ax = plt.gca()\n",
    "plt.xlabel('Number of deception per story', labelpad=10)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 0.8)\n",
    "plt.xticks([0,1,2,3,4])\n",
    "plt.yticks([0,0.1,0.2,0.3,0.4,0.5,0.6,0.7])\n",
    "plt.grid(axis='y', linestyle='dashed', which='both')\n",
    "# plt.box(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['bottom'].set_color('silver')\n",
    "plt.tick_params(axis='x', which='both', bottom=False, pad=0)\n",
    "plt.tick_params(left=False)\n",
    "# plt.yscale('log')\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.45, 1.17), ncol=4, frameon=False)\n",
    "plt.subplots_adjust(bottom=0.15, top=0.87, right=0.98)\n",
    "\n",
    "plt.savefig('data_figs/jointacc_vs_deception.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Analyze correct answers\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "input_folder = 'extracted_response'\n",
    "answer_folder = 'data_ToMh_old'\n",
    "response_folder = 'API_responses'\n",
    "output_folder = 'check_GPT-4'\n",
    "summary_file = 'GPT-4_data.txt'\n",
    "lengths = [1, 2, 3]\n",
    "orders = [0, 1, 2, 3, 4]\n",
    "prompts = ['CoT', 'MC']\n",
    "tells = ['No_Tell', 'Tell']\n",
    "\n",
    "# Count answer occurrence\n",
    "container_nums = 0\n",
    "story_cnt = 0\n",
    "max_container = 0\n",
    "correct_ans_occur = np.zeros(5)\n",
    "cnt_correct = {'No_Tell': np.zeros(5), 'Tell': np.zeros(5)}\n",
    "cnt_correct_same = {'No_Tell': np.zeros(5), 'Tell': np.zeros(5)}\n",
    "cnt_incorrect = {'No_Tell': np.zeros(5), 'Tell': np.zeros(5)}\n",
    "cnt_incorrect_same = {'No_Tell': np.zeros(5), 'Tell': np.zeros(5)}\n",
    "\n",
    "joint_count_correct = {'No_Tell': np.zeros(5), 'Tell': np.zeros(5)}\n",
    "joint_count_correct_first = {'No_Tell': np.zeros(5), 'Tell': np.zeros(5)}\n",
    "count_correct = {'No_Tell': np.zeros(5), 'Tell': np.zeros(5)}\n",
    "count_correct_first = {'No_Tell': np.zeros(5), 'Tell': np.zeros(5)}\n",
    "count_incorrect = {'No_Tell': np.zeros(5), 'Tell': np.zeros(5)}\n",
    "count_incorrect_first = {'No_Tell': np.zeros(5), 'Tell': np.zeros(5)}\n",
    "first_count = np.zeros(5)\n",
    "first_correct_count = np.zeros(5)\n",
    "notfirst_count = np.zeros(5)\n",
    "notfirst_correct_count = np.zeros(5)\n",
    "containers = ['green_box.', 'blue_box.', 'red_box.', 'green_pantry.', 'blue_pantry.', 'red_pantry.', 'green_bathtub.', 'blue_bathtub.', 'red_bathtub.', 'green_envelope.', 'blue_envelope.', 'red_envelope.', 'green_drawer.', 'blue_drawer.', 'red_drawer.', 'green_bottle.', 'blue_bottle.', 'red_bottle.', 'green_cupboard.', 'blue_cupboard.', 'red_cupboard.', 'green_basket.', 'blue_basket.', 'red_basket.', 'green_crate.', 'blue_crate.', 'red_crate.', 'green_suitcase.', 'blue_suitcase.', 'red_suitcase.', 'green_bucket.', 'blue_bucket.', 'red_bucket.', 'green_container.', 'blue_container.', 'red_container.', 'green_treasure_chest.', 'blue_treasure_chest.', 'red_treasure_chest.']\n",
    "\n",
    "\n",
    "for tell, prompt, length in itertools.product(tells, prompts, lengths):\n",
    "    for sample_num in range(1, 21):\n",
    "        correct_so_far = True\n",
    "        prev_ans = None\n",
    "        for order in orders:\n",
    "            input_fn = os.path.join(input_folder, tell, prompt, f'length_{length}', f'sample_{sample_num}',\n",
    "                                    f'order_{order}.txt')\n",
    "            response_fn = os.path.join(response_folder, tell, prompt, f'length_{length}', f'sample_{sample_num}',\n",
    "                                    f'order_{order}.txt')\n",
    "            answer_fn = os.path.join(answer_folder, tell, 'CoT', f'length_{length}', f'sample_{sample_num}',\n",
    "                                        f'order_{order}.txt')\n",
    "            output_fn = os.path.join(output_folder, tell, prompt, f'length_{length}', f'sample_{sample_num}',\n",
    "                                        f'order_{order}.txt')\n",
    "            with open(answer_fn, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "                answer_pool = []\n",
    "                answer_lines = []\n",
    "                story_total_line = len(lines) - 5\n",
    "                splits = np.array_split(list(range(story_total_line)), 5)\n",
    "                for line_num, line in enumerate(lines):\n",
    "                    if 'Answer:' in line.split(' '):\n",
    "                        correct_answer = line.split(' ', 1)[1].split('\\n')[0]\n",
    "                    # check if sentence contains a container\n",
    "                    elif 'likes' not in line.split(' '):\n",
    "                        container = [container for container in line.split() if container in containers]\n",
    "                        answer_pool += container\n",
    "                if order <= 4:\n",
    "                    for line_num, line in enumerate(lines):\n",
    "                        if correct_answer + '.' in line:\n",
    "                            for i, split in enumerate(splits):\n",
    "                                if line_num in split:\n",
    "                                    correct_ans_occur[i] += 1\n",
    "\n",
    "            story_cnt += 1\n",
    "            container_nums += len(answer_pool)\n",
    "            if len(answer_pool) > max_container:\n",
    "                max_container = len(answer_pool)\n",
    "            with open(input_fn, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "                if len(lines[0].split(' ')) == 2:\n",
    "                    answer_line = lines[0].split(' ')[1]\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            # Same-as-last answer count\n",
    "            if order >= 1:\n",
    "                if answer_line == correct_answer and correct_so_far:\n",
    "                    cnt_correct[tell][order] += 1\n",
    "                    if correct_answer == prev_ans:\n",
    "                        cnt_correct_same[tell][order] += 1\n",
    "                elif answer_line != correct_answer:\n",
    "                    cnt_incorrect[tell][order] += 1\n",
    "                    if correct_answer == prev_ans:\n",
    "                        cnt_incorrect_same[tell][order] += 1\n",
    "            prev_ans = correct_answer\n",
    "\n",
    "            # Answer occurrence count\n",
    "            if length in [3]:\n",
    "                if tell == 'No_Tell':\n",
    "                    if correct_answer + '.' in answer_pool[-1]:\n",
    "                        first_count[order] += 1\n",
    "                        if answer_line == correct_answer:\n",
    "                            first_correct_count[order] += 1\n",
    "                    else:\n",
    "                        notfirst_count[order] += 1\n",
    "                        if answer_line == correct_answer:\n",
    "                            notfirst_correct_count[order] += 1\n",
    "\n",
    "            # Ratio of correct answers being the first\n",
    "            if answer_line == correct_answer:\n",
    "                count_correct[tell][order] += 1\n",
    "                if correct_answer + '.' in answer_pool[0]:\n",
    "                    count_correct_first[tell][order] += 1\n",
    "            elif answer_line != correct_answer:\n",
    "                count_incorrect[tell][order] += 1\n",
    "                if answer_line + '.' in answer_pool[0]:\n",
    "                    count_incorrect_first[tell][order] += 1\n",
    "            \n",
    "            # Ratio of joint correct answers being the first\n",
    "            if answer_line == correct_answer and correct_so_far:\n",
    "                joint_count_correct[tell][order] += 1\n",
    "                if correct_answer + '.' in answer_pool[:2]:\n",
    "                    joint_count_correct_first[tell][order] += 1\n",
    "            elif answer_line != correct_answer:\n",
    "                correct_so_far = False\n",
    "\n",
    "for tell in tells:\n",
    "    for order in orders:\n",
    "        print(cnt_correct_same[tell][order] / cnt_correct[tell][order])\n",
    "    for order in orders:\n",
    "        print(cnt_incorrect_same[tell][order] / cnt_incorrect[tell][order])  \n",
    "# print(max_container)\n",
    "# print('avg. container: ', container_nums / story_cnt)\n",
    "# print(correct_ans_occur)\n",
    "# print(first_count)\n",
    "# print('avg:')\n",
    "# print(np.mean(first_correct_count[3:]) / np.mean(first_count[3:]))\n",
    "# print(np.mean(notfirst_correct_count[3:]) / np.mean(notfirst_count[3:]))\n",
    "# print(np.mean(first_correct_count) / np.mean(first_count))\n",
    "# print(np.mean(notfirst_correct_count) / np.mean(notfirst_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot: same-as-last ratio in GPT-4 correct answer\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "FONT_SIZE = 25\n",
    "TICK_SIZE = 21\n",
    "LEGEND_FONT_SIZE = 22\n",
    "\n",
    "plt.rc('font', size=FONT_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=FONT_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=FONT_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=TICK_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=TICK_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=LEGEND_FONT_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=42)  # fontsize of the figure title\n",
    "\n",
    "markers = ['o', 's', '^', 'D', 'o']\n",
    "ratio_same_correct = {'No_Tell': np.zeros(4), 'Tell': np.zeros(4)}\n",
    "ratio_same_incorrect = {'No_Tell': np.zeros(4), 'Tell': np.zeros(4)}\n",
    "\n",
    "for tell, order in itertools.product(tells, range(1,5)):\n",
    "    ratio_same_correct[tell][order-1] = cnt_correct_same[tell][order] / cnt_correct[tell][order]\n",
    "    ratio_same_incorrect[tell][order-1] = cnt_incorrect_same[tell][order] / cnt_incorrect[tell][order]\n",
    "\n",
    "colors = ['#7C84B9', 'orange', 'lightcoral', 'steelblue']\n",
    "plt.figure(figsize=(9, 5.5))\n",
    "for tell in ['Tell']:\n",
    "    plt.plot([1,2,3,4], 100 * ratio_same_correct[tell], linewidth=2.6, marker=markers[0], label='GPT-4 correct answer', markersize=9, color=colors[0])\n",
    "    plt.plot([1,2,3,4], 100 * ratio_same_incorrect[tell], linewidth=2.6, marker=markers[1], label='GPT-4 wrong answer', markersize=9, color=colors[1])\n",
    "\n",
    "# 添加标题和轴标签\n",
    "# plt.title('Multiple Line Plot')\n",
    "ax = plt.gca()\n",
    "plt.xlabel('ToM order', labelpad=10)\n",
    "plt.ylabel('Ratio of\\nsame-as-last-order answers')\n",
    "\n",
    "plt.xticks([1,2,3,4])\n",
    "plt.yticks([10,20,30,40,50,60,70,80,90,100])\n",
    "plt.grid(axis='y', linestyle='dashed', which='both')\n",
    "# plt.box(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['bottom'].set_color('silver')\n",
    "plt.tick_params(axis='x', which='both', bottom=False, pad=0)\n",
    "plt.tick_params(left=False)\n",
    "# plt.yscale('log')\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.45, 1.17), ncol=4, frameon=False)\n",
    "plt.subplots_adjust(bottom=0.15, top=0.87, right=0.98)\n",
    "\n",
    "plt.savefig('data_figs/jointacc_vs_deception.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot: Even distribution of answers\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data to plot\n",
    "labels = ['first fifth', 'second fifth', 'third fifth', 'fourth fifth', 'fifth fifth']\n",
    "colors = ['pink', '#7C84B9', (1, 0.851, 0.4), 'lightcoral', 'steelblue']\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.pie(correct_ans_occur, autopct='%1.1f%%', colors=colors, labeldistance=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot: Ratio of correct/incorrect answer in first-container answers\"\"\"\n",
    "\n",
    "occurrence = ['Last\\ncontainer', 'First\\ncontainer', 'Other\\nplaces']\n",
    "cols = ['#7C84B9', (1, 0.851, 0.4)]\n",
    "\n",
    "# Set the positions of the bars\n",
    "x = np.arange(len(occurrence))\n",
    "width = 0.4\n",
    "\n",
    "No_Tell_ratio = np.array([[0.75, 0.5609756097560976, 0.23333333333333334], [0.3969849246231156, 0.3412698412698413, 0.5555555555555556]])\n",
    "Tell_ratio = np.array([[0.5434782608695652, 0.5280898876404494, 0.2638888888888889], [0.3969849246231156, 0.26, 0.3170731707317073]])\n",
    "# No_Tell_ratio = np.array([[0.675, 0.5575221238938053, 0.39520958083832336], [0.3969849246231156, 0.3412698412698413, 0.5555555555555556]])\n",
    "# Tell_ratio = np.array([[0.8, 0.6867924528301886, 0.5746835443037974], [0.3969849246231156, 0.26, 0.3170731707317073]])\n",
    "first_ratio = []\n",
    "# Create the bar chart\n",
    "plt.bar(x - width/2, No_Tell_ratio[0], width, label='No deception', zorder=2, color=cols[0])\n",
    "# plt.bar(x - width, 1 - No_Tell_ratio[0], width, label='Step-by-step prompting', zorder=2, color=cols[0][0])\n",
    "plt.bar(x + width/2, Tell_ratio[0], width, label='Exists deception', zorder=2, color=cols[1])\n",
    "# plt.bar(x + 2*width, 1 - No_Tell_ratio[1], width, label='Step-by-step prompting', zorder=2, color=cols[1][0])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Occurrence of the correct answer')\n",
    "plt.ylabel('GPT-4 Accuracy')\n",
    "plt.grid(axis='y', linestyle='dashed', zorder=1)\n",
    "# plt.title('Bar Chart with Two Columns')\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "\n",
    "# Set the x-axis tick labels\n",
    "plt.xticks(x, occurrence)\n",
    "plt.yticks([0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8])\n",
    "plt.tick_params(bottom=False, left=False)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=3, frameon=False)\n",
    "plt.subplots_adjust(bottom=0.15)\n",
    "\n",
    "# Display the plot\n",
    "plt.savefig(f'data_figs/acc_vs_occurrence.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot ratio of first container in correct answers\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "FONT_SIZE = 25\n",
    "TICK_SIZE = 21\n",
    "LEGEND_FONT_SIZE = 22\n",
    "\n",
    "plt.rc('font', size=FONT_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=FONT_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=FONT_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=TICK_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=TICK_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=LEGEND_FONT_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=42)  # fontsize of the figure title\n",
    "\n",
    "markers = ['o', '.', '^', 'D', 'o']\n",
    "ratio_first = {'No_Tell': np.zeros(5), 'Tell': np.zeros(5)}\n",
    "ratio_first_incorrect = {'No_Tell': np.zeros(5), 'Tell': np.zeros(5)}\n",
    "joint_ratio_first = {'No_Tell': np.zeros(5), 'Tell': np.zeros(5)}\n",
    "\n",
    "\n",
    "for tell, order in itertools.product(tells, orders):\n",
    "    ratio_first[tell][order] = count_correct_first[tell][order] / count_correct[tell][order]\n",
    "    ratio_first_incorrect[tell][order] = count_incorrect_first[tell][order] / count_incorrect[tell][order]\n",
    "    joint_ratio_first[tell][order] = joint_count_correct_first[tell][order] / joint_count_correct[tell][order]\n",
    "\n",
    "colors = ['#7C84B9', 'orange', 'lightcoral', 'steelblue']\n",
    "plt.figure(figsize=(9, 5.5)) \n",
    "for i, tell in enumerate(tells):\n",
    "    # plt.plot([0,1,2,3,4], joint_ratio_first[tell], linewidth=2.6, marker=markers[i], label=tell+'_joint', markersize=9)\n",
    "    plt.plot([0,1,2,3,4], ratio_first[tell], linewidth=2.6, marker=markers[i], label=tell, markersize=9)\n",
    "    plt.plot([0,1,2,3,4], ratio_first_incorrect[tell], linewidth=2.6, marker=markers[i], label=tell+'_incorrect', markersize=9)\n",
    "\n",
    "# 添加标题和轴标签\n",
    "# plt.title('Multiple Line Plot')\n",
    "ax = plt.gca()\n",
    "plt.xlabel('ToM order', labelpad=10)\n",
    "plt.ylabel('Ratio')\n",
    "\n",
    "plt.xticks([0,1,2,3,4])\n",
    "# plt.yticks([0,0.1,0.2,0.3,0.4,0.5,0.6,0.7])\n",
    "plt.grid(axis='y', linestyle='dashed', which='both')\n",
    "# plt.box(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(True)\n",
    "ax.spines['bottom'].set_color('silver')\n",
    "plt.tick_params(axis='x', which='both', bottom=False, pad=0)\n",
    "plt.tick_params(left=False)\n",
    "# plt.yscale('log')\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.45, 1.17), ncol=4, frameon=False)\n",
    "plt.subplots_adjust(bottom=0.15, top=0.87, right=0.98)\n",
    "\n",
    "plt.savefig('data_figs/jointacc_vs_deception.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
