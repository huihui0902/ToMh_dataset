{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens:  543968\n",
      "Avg. num of tokens:  453.3066666666667\n",
      "Max tokens:  624\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Count tokens.\"\"\"\n",
    "import os\n",
    "import nltk\n",
    "import itertools\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "folder = 'data_ToMh_old'\n",
    "lengths = [1, 2, 3]\n",
    "orders = [0, 1, 2, 3, 4]\n",
    "prompts = ['CoT', 'MC']\n",
    "tells = ['No_Tell', 'Tell']\n",
    "\n",
    "count_stories = 0\n",
    "count_tokens = 0\n",
    "max_token = 0\n",
    "\n",
    "for tell, prompt, length, order, sample_num in itertools.product(tells, prompts, lengths, orders, range(1, 21)):\n",
    "    filename = os.path.join(folder, tell, prompt, f'length_{length}', f'sample_{sample_num}', f'order_{order}.txt')\n",
    "    # Open and read the txt file:\n",
    "    count_stories += 1\n",
    "    with open(filename, 'r') as file:\n",
    "        data = file.read()\n",
    "    # Tokenize the data:\n",
    "    tokens = word_tokenize(data)\n",
    "    if len(tokens) > max_token:\n",
    "        max_token = len(tokens)\n",
    "\n",
    "    count_tokens += len(tokens)\n",
    "\n",
    "# Print the number of tokens:\n",
    "print(\"Number of tokens: \", count_tokens)\n",
    "print(\"Avg. num of tokens: \", count_tokens / count_stories)\n",
    "print(\"Max tokens: \", max_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story num:  1200\n",
      "Avg. num of lines:  26.466666666666665\n",
      "Number of tokens:  291190\n",
      "Avg. num of tokens:  242.65833333333333\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Count story and lines.\"\"\"\n",
    "import os\n",
    "import nltk\n",
    "import itertools\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "count_stories = 0\n",
    "count_lines = 0\n",
    "count_tokens = 0\n",
    "prev_linenum = None\n",
    "\n",
    "folder = 'data_ToMh'\n",
    "lengths = [1, 2, 3]\n",
    "orders = [0, 1, 2, 3, 4]\n",
    "prompts = ['CoT', 'MC']\n",
    "tells = ['No_Tell', 'Tell']\n",
    "\n",
    "\n",
    "for tell, prompt, length, order, sample_num in itertools.product(tells, prompts, lengths, orders, range(1, 21)):\n",
    "    filename = os.path.join(folder, tell, prompt, f'length_{length}', f'sample_{sample_num}', f'order_{order}.txt')\n",
    "    # Open and read the txt file:\n",
    "    count_stories += 1\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            if line.split(' ', 1)[0].isdigit():\n",
    "                count_lines += 1\n",
    "                count_tokens += len(word_tokenize(line))\n",
    "        \n",
    "print(\"Story num: \", count_stories)\n",
    "print(\"Avg. num of lines: \", count_lines / count_stories)\n",
    "print(\"Number of tokens: \", count_tokens)\n",
    "print(\"Avg. num of tokens: \", count_tokens / count_stories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
